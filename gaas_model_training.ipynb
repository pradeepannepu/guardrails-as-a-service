{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9HghmiUAOQn",
        "outputId": "97a028e4-d13e-40d9-85df-391bfe7d0bb4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntM-i_J95wsg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "S024fX1k6xBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALLOWED_KEYWORDS = [\n",
        "    # Core networking\n",
        "    \"tcp\", \"udp\", \"ip\", \"bgp\", \"ospf\", \"isis\", \"mpls\", \"vxlan\", \"evpn\",\n",
        "    \"ethernet\", \"mac\", \"arp\", \"dhcp\", \"dns\", \"nat\", \"acl\", \"subnet\",\n",
        "    \"routing\", \"switching\", \"firewall\", \"load balancer\", \"sdn\", \"segment routing\",\n",
        "    # Interconnection / peering / datacenter\n",
        "    \"datacenter\", \"leaf spine\", \"underlay\", \"overlay\", \"spine\", \"leaf\",\n",
        "    \"rdma\", \"roce\", \"fabric\", \"top of rack\", \"tor\", \"edge\", \"colo\",\n",
        "    \"interconnect\", \"peering\", \"ixp\", \"lag\", \"link aggregation\", \"qos\",\n",
        "    \"throughput\", \"latency\", \"packet\", \"mtu\", \"jumbo frame\", \"flow control\",\n",
        "    \"netflow\", \"sflow\", \"telemetry\", \"gre\", \"ipsec\", \"tls\", \"ssl\", \"anycast\",\n",
        "    \"multicast\", \"unicast\", \"broadcast\", \"fhrp\", \"hsrp\", \"vrrp\", \"glbp\",\n",
        "    \"l2\", \"l3\", \"layer 2\", \"layer 3\", \"sd-wan\", \"wan\", \"lan\", \"vxlan evpn\",\n",
        "]\n",
        "\n",
        "REFUSAL_MESSAGE = (\n",
        "    \"I am restricted to Equinix Related stuff dude. \"\n",
        "    \"Your request appears out of scope. Please rephrase within that domain.\"\n",
        ")\n",
        "\n",
        "def is_in_scope(text: str) -> bool:\n",
        "    t = text.lower()\n",
        "    hits = sum(1 for k in ALLOWED_KEYWORDS if k in t)\n",
        "    # Threshold: at least 1 direct keyword, can tune later\n",
        "    return hits > 0"
      ],
      "metadata": {
        "id": "ltZoK06O57Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Sample:\n",
        "    instruction: str\n",
        "    response: str\n",
        "\n",
        "class GuardDataset(Dataset):\n",
        "    def __init__(self, samples: List[Sample], tokenizer: AutoTokenizer, max_len: int = 1024):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        # Simple instruction formatting (adjust to base model's chat template if needed)\n",
        "        prompt = f\"Instruction: {s.instruction}\\nResponse:\"\n",
        "        full = prompt + \" \" + s.response\n",
        "        tokens = self.tokenizer(\n",
        "            full,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = tokens[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = tokens[\"attention_mask\"].squeeze(0)\n",
        "        labels = input_ids.clone()\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "XUiIW43u59he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------\n",
        "# Synthetic Data Helpers (placeholder â€” replace with real curated data)\n",
        "# ------------------------\n",
        "\n",
        "import random\n",
        "\n",
        "POSITIVE_EXAMPLES = [\n",
        "    (\"Explain the difference between OSPF and BGP.\",\n",
        "     \"OSPF is a link-state IGP optimizing intra-domain convergence; BGP is a path-vector EGP handling inter-domain policy-based routing.\"),\n",
        "    (\"What is VXLAN EVPN used for in datacenters?\",\n",
        "     \"VXLAN with EVPN control plane provides scalable L2/L3 segmentation over an IP underlay using BGP for MAC+IP advertisement.\"),\n",
        "    (\"How does a leaf-spine architecture improve scalability?\",\n",
        "     \"It provides predictable latency and equal-cost multi-pathing (ECMP) between any two endpoints by using a non-blocking fabric.\"),\n",
        "    (\"Describe typical datacenter fabric telemetry approaches.\",\n",
        "     \"Operators combine streaming telemetry (gNMI), flow data (NetFlow/sFlow/IPFIX), and in-band network telemetry for real-time visibility.\"),\n",
        "]\n",
        "\n",
        "NEGATIVE_PROMPTS = [\n",
        "    \"Write a poem about the sunrise.\",\n",
        "    \"Explain quantum entanglement.\",\n",
        "    \"Give a recipe for lasagna.\",\n",
        "    \"Summarize a medieval history event.\",\n",
        "    \"Discuss stock trading strategies.\",\n",
        "    \"How to bake sourdough bread?\",\n",
        "    \"Tell me a joke about cats.\",\n",
        "]\n",
        "\n",
        "def build_training_samples(positive_scale: int = 1, negative_scale: int = 2) -> List[Sample]:\n",
        "    samples: List[Sample] = []\n",
        "    for inst, resp in POSITIVE_EXAMPLES * positive_scale:\n",
        "        samples.append(Sample(inst, resp))\n",
        "    # Negative become refusal responses\n",
        "    for p in NEGATIVE_PROMPTS * negative_scale:\n",
        "        samples.append(Sample(p, REFUSAL_MESSAGE))\n",
        "    random.shuffle(samples)\n",
        "    return samples"
      ],
      "metadata": {
        "id": "Dk6rhN8D5_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenizer(model_name: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "def prepare_model(model_name: str, lora_r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.05):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=lora_r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n",
        "\n",
        "def train(\n",
        "    model_name: str,\n",
        "    output_dir: str,\n",
        "    epochs: int = 2,\n",
        "    batch_size: int = 4,\n",
        "    lr: float = 2e-4,\n",
        "    max_len: int = 1024,\n",
        "):\n",
        "    tokenizer = load_tokenizer(model_name)\n",
        "    samples = build_training_samples()\n",
        "    dataset = GuardDataset(samples, tokenizer, max_len=max_len)\n",
        "\n",
        "    model = prepare_model(model_name)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.05,\n",
        "        learning_rate=lr,\n",
        "        logging_steps=10,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        bf16=torch.cuda.is_available(),\n",
        "        fp16=False,\n",
        "        weight_decay=0.01,\n",
        "        report_to=[],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Training complete. Adapter saved at:\", output_dir)"
      ],
      "metadata": {
        "id": "yL9OKWTb6A4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    model_name=\"google/gemma-3-270m\",\n",
        "    output_dir=\"./guard_adapter-mini\",\n",
        "    epochs=2,\n",
        "    batch_size=4,\n",
        "    lr=2e-4,\n",
        "    max_len=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "bmKSQWMg6CQs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "akzil0gx6D8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def load_tokenizer(model_name: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "class GuardedModel:\n",
        "    def __init__(self, base_model: str, adapter_path: Optional[str] = None, temperature: float = 0.2):\n",
        "        self.tokenizer = load_tokenizer(base_model)\n",
        "        if adapter_path and os.path.exists(adapter_path):\n",
        "            # Load adapter\n",
        "            base = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "            self.model = PeftModel.from_pretrained(base, adapter_path)\n",
        "        else:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def generate_guarded(self, prompt: str, max_new_tokens: int = 256) -> str:\n",
        "        if not is_in_scope(prompt):\n",
        "            return REFUSAL_MESSAGE\n",
        "        formatted = f\"Instruction: {prompt}\\nResponse:\"\n",
        "        inputs = self.tokenizer(\n",
        "            formatted,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = {k: v.to(self.model.device) for k in inputs.keys()}\n",
        "        with torch.no_grad():\n",
        "            out = self.model.generate(\n",
        "                **inputs,\n",
        "                do_sample=self.temperature > 0,\n",
        "                temperature=self.temperature,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "        text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        # Post-trim: extract after \"Response:\"\n",
        "        if \"Response:\" in text:\n",
        "            text = text.split(\"Response:\", 1)[1].strip()\n",
        "        # Safety fallback\n",
        "        if not is_in_scope(text):\n",
        "            return REFUSAL_MESSAGE\n",
        "        return text"
      ],
      "metadata": {
        "id": "RXTBJx8y6F7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GuardedModel(base_model=\"unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit\", adapter_path=\"./guard_adapter-mini\", temperature=0.2)"
      ],
      "metadata": {
        "id": "kkZ2PkOM6IRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm.generate_guarded(\"write a Shakespeare style poem\")"
      ],
      "metadata": {
        "id": "YkdkKzJO6Jw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}